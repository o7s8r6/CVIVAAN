{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/o7s8r6/CVIVAAN/blob/main/micrograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEYvcfHRB8dw"
      },
      "source": [
        "![pyimagesearch_university_logo.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4RpK9pawQzP"
      },
      "source": [
        "# Automatic Differentiation Part 2: Implementation Using Micrograd\n",
        "### by [PyImageSearch.com](http://www.pyimagesearch.com)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ntZ1AkXZIxY"
      },
      "source": [
        "## Welcome to **[PyImageSearch University](https://pyimg.co/university)** Jupyter Notebooks!\n",
        "\n",
        "This notebook is associated with the [Automatic Differentiation Part 2: Implementation Using Micrograd](https://pyimg.co/ra6ow)  blog post published on 2022-12-26.\n",
        "\n",
        "Only the code for the blog post is here. Most codeblocks have a 1:1 relationship with what you find in the blog post with two exceptions: (1) Python classes are not separate files as they are typically organized with PyImageSearch projects, and (2) Command Line Argument parsing is replaced with an `args` dictionary that you can manipulate as needed.\n",
        "\n",
        "We recommend that you execute (press ▶️) the code block-by-block, as-is, before adjusting parameters and `args` inputs. Once you've verified that the code is working, you are welcome to hack with it and learn from manipulating inputs, settings, and parameters. For more information on using Jupyter and Colab, please refer to these resources:\n",
        "\n",
        "*   [Jupyter Notebook User Interface](https://jupyter-notebook.readthedocs.io/en/stable/notebook.html#notebook-user-interface)\n",
        "*   [Overview of Google Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "As a reminder, these PyImageSearch University Jupyter Notebooks are not for sharing; please refer to the **Copyright** directly below and **Code License Agreement** in the last cell of this notebook. \n",
        "\n",
        "Happy hacking!\n",
        "\n",
        "*PyImageSearch Team*\n",
        "\n",
        "<hr>\n",
        "\n",
        "***Copyright:*** *The contents of this Jupyter Notebook, unless otherwise indicated, are Copyright 2022 OptiReto, LLC and PyImageSearch.com. All rights reserved. Content like this is made possible by the time invested by the authors. If you received this Jupyter Notebook and did not purchase it, please consider making future content possible by joining PyImageSearch University at https://pyimg.co/university today.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SgTVT3HagGZ"
      },
      "source": [
        "## Blog Post Code"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports and Setup"
      ],
      "metadata": {
        "id": "cnUkZ8W-OAGR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCJRV7q4VvWu"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "from typing import List, Tuple, Union\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The `Value` class\n"
      ],
      "metadata": {
        "id": "h-7cWdMeHkNv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PejtrKqdHFZ6"
      },
      "outputs": [],
      "source": [
        "class Value(object):\n",
        "    \"\"\"\n",
        "    We need to wrap the raw data into a class that will store the\n",
        "    metadata to help in automatic differentiation.\n",
        "\n",
        "    Attributes:\n",
        "        data (float): The data for the Value node.\n",
        "        _children (Tuple): The children of the current node.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data: float, _children: Tuple = ()):\n",
        "        # The raw data for the Value node.\n",
        "        self.data = data\n",
        "\n",
        "        # The partial gradient of the last node with respect to this\n",
        "        # node. This is also termed as the global gradient.\n",
        "        # Gradient 0.0 means that there is no effect of the change\n",
        "        # of the last node with respect to this node. On\n",
        "        # initialization it is assumed that all the variables have no\n",
        "        # effect on the entire architecture.\n",
        "        self.grad = 0.0\n",
        "\n",
        "        # The function that derives the gradient of the children nodes\n",
        "        # of the current node. It is easier this way, because each node\n",
        "        # is built from children nodes and an operation. Upon back-propagation\n",
        "        # the current node can easily fill in the gradients of the children.\n",
        "        # Note: The global gradient is the multiplication of the local gradient\n",
        "        # and the flowing gradient from the parent.\n",
        "        self._backward = lambda: None\n",
        "\n",
        "        # Define the children of this node.\n",
        "        self._prev = set(_children)\n",
        "\n",
        "    def __repr__(self):\n",
        "        # This is the string representation of the Value node.\n",
        "        return f\"Value(data={self.data}, grad={self.grad})\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a Value node\n",
        "raw_data = 5.0\n",
        "print(f\"Raw Data(data={raw_data}, type={type(raw_data)}\")\n",
        "value_node = Value(data=raw_data)\n",
        "\n",
        "# Calling the `__repr__` function here\n",
        "print(value_node)"
      ],
      "metadata": {
        "id": "DpouoE6ZJYkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Addition"
      ],
      "metadata": {
        "id": "byzDN5J2KfNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_addition(self, other: Union[\"Value\", float]) -> \"Value\":\n",
        "    \"\"\"\n",
        "    The addition operation for the Value class.\n",
        "    Args:\n",
        "        other (Union[\"Value\", float]): The other value to add to this one.\n",
        "    Usage:\n",
        "        >>> x = Value(2)\n",
        "        >>> y = Value(3)\n",
        "        >>> z = x + y\n",
        "        >>> z.data\n",
        "        5\n",
        "    \"\"\"\n",
        "    # If the other value is not a Value, then we need to wrap it.\n",
        "    other = other if isinstance(other, Value) else Value(other)\n",
        "\n",
        "    # Create a new Value node that will be the output of the addition.\n",
        "    out = Value(data=self.data + other.data, _children=(self, other))\n",
        "\n",
        "    def _backward():\n",
        "        # Local gradient:\n",
        "        # x = a + b\n",
        "        # dx/da = 1\n",
        "        # dx/db = 1\n",
        "        # Global gradient with chain rule:\n",
        "        # dy/da = dy/dx . dx/da = dy/dx . 1\n",
        "        # dy/db = dy/dx . dx/db = dy/dx . 1\n",
        "        self.grad += out.grad * 1.0\n",
        "        other.grad += out.grad * 1.0\n",
        "\n",
        "    # Set the backward function on the output node.\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "def custom_reverse_addition(self, other):\n",
        "    \"\"\"\n",
        "    Reverse addition operation for the Value class.\n",
        "    Args:\n",
        "        other (float): The other value to add to this one.\n",
        "    Usage:\n",
        "        >>> x = Value(2)\n",
        "        >>> y = Value(3)\n",
        "        >>> z = y + x\n",
        "        >>> z.data\n",
        "        5\n",
        "    \"\"\"\n",
        "    # This is the same as adding. We can reuse the __add__ method.\n",
        "    return self + other\n",
        "\n",
        "\n",
        "Value.__add__ = custom_addition\n",
        "Value.__radd__ = custom_reverse_addition"
      ],
      "metadata": {
        "id": "fzUDpllTzPzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a and b\n",
        "a = Value(data=5.0)\n",
        "b = Value(data=6.0)\n",
        "\n",
        "# Print the addition\n",
        "print(f\"{a} + {b} => {a+b}\")"
      ],
      "metadata": {
        "id": "jN0diL4s8sQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a and b\n",
        "c = a + b\n",
        "\n",
        "# Assign a global gradient to c\n",
        "c.grad = 11.0\n",
        "print(f\"c => {c}\")\n",
        "\n",
        "# Now apply `_backward` to c\n",
        "c._backward()\n",
        "print(f\"a => {a}\")\n",
        "print(f\"b => {b}\")"
      ],
      "metadata": {
        "id": "WArfAObw9X3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Note that the global gradient of $c$ is routed to $a$ and $b$."
      ],
      "metadata": {
        "id": "zVI4XgJ19qnP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiplication"
      ],
      "metadata": {
        "id": "nUsMZqEv_wx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_multiplication(self, other: Union[\"Value\", float]) -> \"Value\":\n",
        "    \"\"\"\n",
        "    The multiplication operation for the Value class.\n",
        "    Args:\n",
        "        other (float): The other value to multiply to this one.\n",
        "    Usage:\n",
        "        >>> x = Value(2)\n",
        "        >>> y = Value(3)\n",
        "        >>> z = x * y\n",
        "        >>> z.data\n",
        "        6\n",
        "    \"\"\"\n",
        "    # If the other value is not a Value, then we need to wrap it.\n",
        "    other = other if isinstance(other, Value) else Value(other)\n",
        "\n",
        "    # Create a new Value node that will be the output of\n",
        "    # the multiplication.\n",
        "    out = Value(data=self.data * other.data, _children=(self, other))\n",
        "\n",
        "    def _backward():\n",
        "        # Local gradient:\n",
        "        # x = a * b\n",
        "        # dx/da = b\n",
        "        # dx/db = a\n",
        "        # Global gradient with chain rule:\n",
        "        # dy/da = dy/dx . dx/da = dy/dx . b\n",
        "        # dy/db = dy/dx . dx/db = dy/dx . a\n",
        "        self.grad += out.grad * other.data\n",
        "        other.grad += out.grad * self.data\n",
        "\n",
        "    # Set the backward function on the output node.\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "def custom_reverse_multiplication(self, other):\n",
        "    \"\"\"\n",
        "    Reverse multiplication operation for the Value class.\n",
        "    Args:\n",
        "        other (float): The other value to multiply to this one.\n",
        "    Usage:\n",
        "        >>> x = Value(2)\n",
        "        >>> y = Value(3)\n",
        "        >>> z = y * x\n",
        "        >>> z.data\n",
        "        6\n",
        "    \"\"\"\n",
        "    # This is the same as multiplying. We can reuse the __mul__ method.\n",
        "    return self * other\n",
        "\n",
        "\n",
        "Value.__mul__ = custom_multiplication\n",
        "Value.__rmul__ = custom_reverse_multiplication"
      ],
      "metadata": {
        "id": "5gjJhjUS_1qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a and b\n",
        "a = Value(data=5.0)\n",
        "b = Value(data=6.0)\n",
        "\n",
        "# Print the multiplication\n",
        "print(f\"{a} * {b} => {a*b}\")"
      ],
      "metadata": {
        "id": "AHWQW-smCCFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiply a and b\n",
        "c = a * b\n",
        "\n",
        "# Assign a global gradient to c\n",
        "c.grad = 11.0\n",
        "print(f\"c => {c}\")\n",
        "\n",
        "# Now apply `_backward` to c\n",
        "c._backward()\n",
        "print(f\"a => {a}\")\n",
        "print(f\"b => {b}\")"
      ],
      "metadata": {
        "id": "Mr6kE2AxCEe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Power"
      ],
      "metadata": {
        "id": "a9dSZ79BC8WG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_power(self, other):\n",
        "    \"\"\"\n",
        "    The power operation for the Value class.\n",
        "    Args:\n",
        "        other (float): The other value to raise this one to.\n",
        "    Usage:\n",
        "        >>> x = Value(2)\n",
        "        >>> z = x ** 2.0\n",
        "        >>> z.data\n",
        "        4\n",
        "    \"\"\"\n",
        "    assert isinstance(\n",
        "        other, (int, float)\n",
        "    ), \"only supporting int/float powers for now\"\n",
        "\n",
        "    # Create a new Value node that will be the output of the power.\n",
        "    out = Value(data=self.data ** other, _children=(self,))\n",
        "\n",
        "    def _backward():\n",
        "        # Local gradient:\n",
        "        # x = a ** b\n",
        "        # dx/da = b * a ** (b - 1)\n",
        "        # Global gradient:\n",
        "        # dy/da = dy/dx . dx/da = dy/dx . b * a ** (b - 1)\n",
        "        self.grad += out.grad * (other * self.data ** (other - 1))\n",
        "\n",
        "    # Set the backward function on the output node.\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "\n",
        "Value.__pow__ = custom_power"
      ],
      "metadata": {
        "id": "E009DiC8Chqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a\n",
        "a = Value(data=5.0)\n",
        "# For power operation we will use\n",
        "# the raw data and not wrap it into\n",
        "# a node. This is done for simplicity.\n",
        "b = 2.0\n",
        "\n",
        "# Print the power operation\n",
        "print(f\"{a} ** {b} => {a**b}\")"
      ],
      "metadata": {
        "id": "H9MtELiTaZQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Raise a to the power of b\n",
        "c = a ** b\n",
        "\n",
        "# Assign a global gradient to c\n",
        "c.grad = 11.0\n",
        "print(f\"c => {c}\")\n",
        "\n",
        "# Now apply `_backward` to c\n",
        "c._backward()\n",
        "print(f\"a => {a}\")\n",
        "print(f\"b => {b}\")"
      ],
      "metadata": {
        "id": "61nLQh_AabTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Negation"
      ],
      "metadata": {
        "id": "2hwVHW2xC935"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_negation(self):\n",
        "    \"\"\"\n",
        "    Negation operation for the Value class.\n",
        "    Usage:\n",
        "        >>> x = Value(2)\n",
        "        >>> z = -x\n",
        "        >>> z.data\n",
        "        -2\n",
        "    \"\"\"\n",
        "    # This is the same as multiplying by -1. We can reuse the\n",
        "    # __mul__ method.\n",
        "    return self * -1\n",
        "\n",
        "Value.__neg__ = custom_negation"
      ],
      "metadata": {
        "id": "gKtMPeaRHgec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build `a`\n",
        "a = Value(data=5.0)\n",
        "\n",
        "# Print the negation\n",
        "print(f\"Negation of {a} => {(-a)}\")"
      ],
      "metadata": {
        "id": "s5Yj7Rs1a-oB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Negate a\n",
        "c = -a\n",
        "\n",
        "# Assign a global gradient to c\n",
        "c.grad = 11.0\n",
        "print(f\"c => {c}\")\n",
        "\n",
        "# Now apply `_backward` to c\n",
        "c._backward()\n",
        "print(f\"a => {a}\")"
      ],
      "metadata": {
        "id": "36MKTmY_ci13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Subtraction"
      ],
      "metadata": {
        "id": "JVjXcDzAC_0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_subtraction(self, other):\n",
        "    \"\"\"\n",
        "    Subtraction operation for the Value class.\n",
        "    Args:\n",
        "        other (float): The other value to subtract to this one.\n",
        "    Usage:\n",
        "        >>> x = Value(2)\n",
        "        >>> y = Value(3)\n",
        "        >>> z = x - y\n",
        "        >>> z.data\n",
        "        -1\n",
        "    \"\"\"\n",
        "    # This is the same as adding the negative of the other value.\n",
        "    # We can reuse the __add__ and the __neg__ methods.\n",
        "    return self + (-other)\n",
        "\n",
        "def custom_reverse_subtraction(self, other):\n",
        "    \"\"\"\n",
        "    Reverse subtraction operation for the Value class.\n",
        "    Args:\n",
        "        other (float): The other value to subtract to this one.\n",
        "    Usage:\n",
        "        >>> x = Value(2)\n",
        "        >>> y = Value(3)\n",
        "        >>> z = y - x\n",
        "        >>> z.data\n",
        "        1\n",
        "    \"\"\"\n",
        "    # This is the same as subtracting. We can reuse the __sub__ method.\n",
        "    return other + (-self)\n",
        "\n",
        "\n",
        "Value.__sub__ = custom_subtraction\n",
        "Value.__rsub__ = custom_reverse_subtraction"
      ],
      "metadata": {
        "id": "DG5H7hyKCtRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a and b\n",
        "a = Value(data=5.0)\n",
        "b = Value(data=4.0)\n",
        "\n",
        "# Print the negation\n",
        "print(f\"{a} - {b} => {(a-b)}\")"
      ],
      "metadata": {
        "id": "fAMZeCnkchF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Subtract b from a\n",
        "c = a - b\n",
        "\n",
        "# Assign a global gradient to c\n",
        "c.grad = 11.0\n",
        "print(f\"c => {c}\")\n",
        "\n",
        "# Now apply `_backward` to c\n",
        "c._backward()\n",
        "print(f\"a => {a}\")\n",
        "print(f\"b => {b}\")"
      ],
      "metadata": {
        "id": "8oyKqH95c9zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Division"
      ],
      "metadata": {
        "id": "eFH-SHEHDBPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_division(self, other):\n",
        "    \"\"\"\n",
        "    Division operation for the Value class.\n",
        "    Args:\n",
        "        other (float): The other value to divide to this one.\n",
        "    Usage:\n",
        "        >>> x = Value(10)\n",
        "        >>> y = Value(5)\n",
        "        >>> z = x / y\n",
        "        >>> z.data\n",
        "        2\n",
        "    \"\"\"\n",
        "    # Use the __pow__ method to implement division.\n",
        "    return self * other ** -1\n",
        "\n",
        "def custom_reverse_division(self, other):\n",
        "    \"\"\"\n",
        "    Reverse division operation for the Value class.\n",
        "    Args:\n",
        "        other (float): The other value to divide to this one.\n",
        "    Usage:\n",
        "        >>> x = Value(10)\n",
        "        >>> y = Value(5)\n",
        "        >>> z = y / x\n",
        "        >>> z.data\n",
        "        0.5\n",
        "    \"\"\"\n",
        "    # Use the __pow__ method to implement division.\n",
        "    return other * self ** -1\n",
        "\n",
        "\n",
        "Value.__truediv__ = custom_division\n",
        "Value.__rtruediv__ = custom_reverse_division"
      ],
      "metadata": {
        "id": "icbiLv2aCo0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a and b\n",
        "a = Value(data=6.0)\n",
        "b = Value(data=3.0)\n",
        "\n",
        "# Print the negation\n",
        "print(f\"{a} / {b} => {(a/b)}\")"
      ],
      "metadata": {
        "id": "WdkSBFYGqUM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide a with b\n",
        "c = a / b\n",
        "\n",
        "# Assign a global gradient to c\n",
        "c.grad = 11.0\n",
        "print(f\"c => {c}\")\n",
        "\n",
        "# Now apply `_backward` to c\n",
        "c._backward()\n",
        "print(f\"a => {a}\")\n",
        "print(f\"b => {b}\")"
      ],
      "metadata": {
        "id": "HarPt4ONqoQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> With division we see a same problem with gradient flow as we had seen with subtraction. Have you figured out the problem yet? "
      ],
      "metadata": {
        "id": "_dQLd0m9Hhni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"yellow\">Re</font>ctified <font color=\"yellow\">L</font>inear <font color=\"yellow\">U</font>nit"
      ],
      "metadata": {
        "id": "8g1YPCjgDCtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(self):\n",
        "    \"\"\"\n",
        "    The relu activation function.\n",
        "    Usage:\n",
        "        >>> x = Value(-2)\n",
        "        >>> y = x.relu()\n",
        "        >>> y.data\n",
        "        0\n",
        "    \"\"\"\n",
        "    out = Value(data=0 if self.data < 0 else self.data, _children=(self,))\n",
        "\n",
        "    def _backward():\n",
        "        # Local gradient:\n",
        "        # x = relu(a)\n",
        "        # dx/da = 0 if a < 0 else 1\n",
        "        # Global gradient:\n",
        "        # dy/da = dy/dx . dx/da = dy/dx . (0 if a < 0 else 1)\n",
        "        self.grad += out.grad * (out.data > 0)\n",
        "\n",
        "    # Set the backward function on the output node.\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "\n",
        "Value.relu = relu"
      ],
      "metadata": {
        "id": "5gZNN5nJCzb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a\n",
        "a = Value(data=6.0)\n",
        "\n",
        "# Print a and the negation\n",
        "print(f\"ReLU ({a}) => {(a.relu())}\")\n",
        "print(f\"ReLU (-{a}) => {((-a).relu())}\")"
      ],
      "metadata": {
        "id": "KZIqS_mavMRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a and b\n",
        "a = Value(3.0)\n",
        "b = Value(-3.0)\n",
        "\n",
        "# Apply relu on both the nodes\n",
        "relu_a = a.relu()\n",
        "relu_b = b.relu()\n",
        "\n",
        "# Assign a global gradients\n",
        "relu_a.grad = 11.0\n",
        "relu_b.grad = 11.0\n",
        "\n",
        "# Now apply `_backward`\n",
        "relu_a._backward()\n",
        "print(f\"a => {a}\")\n",
        "relu_b._backward()\n",
        "print(f\"b => {b}\")"
      ],
      "metadata": {
        "id": "p86imP8Vv1EX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The global backward method"
      ],
      "metadata": {
        "id": "bKPoritGDSWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward(self):\n",
        "    \"\"\"\n",
        "    The backward pass of the backward propagation algorithm.\n",
        "    Usage:\n",
        "        >>> x = Value(2)\n",
        "        >>> y = Value(3)\n",
        "        >>> z = x * y\n",
        "        >>> z.backward()\n",
        "        >>> x.grad\n",
        "        3\n",
        "        >>> y.grad\n",
        "        2\n",
        "    \"\"\"\n",
        "    # Build an empty list which will hold the\n",
        "    # topologically sorted graph\n",
        "    topo = []\n",
        "\n",
        "    # Build a set of all the visited nodes\n",
        "    visited = set()\n",
        "\n",
        "    # A closure to help build the topologically sorted graph\n",
        "    def build_topo(node: \"Value\"):\n",
        "        if node not in visited:\n",
        "            # If node is not visited add the node to the\n",
        "            # visited set.\n",
        "            visited.add(node)\n",
        "\n",
        "            # Iterate over the children of the node that\n",
        "            # is being visited\n",
        "            for child in node._prev:\n",
        "                # Apply recursion to build the topologically sorted\n",
        "                # graph of the children\n",
        "                build_topo(child)\n",
        "            \n",
        "            # Only append node to the topologically sorted list\n",
        "            # if all its children are visited.\n",
        "            topo.append(node)\n",
        "\n",
        "    # Call the `build_topo` method on self\n",
        "    build_topo(self)\n",
        "\n",
        "    # Go one node at a time and apply the chain rule\n",
        "    # to get its gradient\n",
        "    self.grad = 1.0\n",
        "    for node in reversed(topo):\n",
        "        node._backward()\n",
        "\n",
        "Value.backward = backward"
      ],
      "metadata": {
        "id": "_6kJxncpC3Yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now create an expression that uses a lot of\n",
        "# primitive operations\n",
        "a = Value(2.0)\n",
        "b = Value(3.0)\n",
        "\n",
        "c = a+b\n",
        "\n",
        "d = 4.0\n",
        "e = c**d\n",
        "\n",
        "f = Value(6.0)\n",
        "g = e/f\n",
        "\n",
        "print(\"BEFORE backward\")\n",
        "for element in [a, b, c, d, e, f, g]:\n",
        "    print(element)\n",
        "\n",
        "# Backward on the final node will backprop\n",
        "# the gradients through the entire DAG\n",
        "g.backward()\n",
        "\n",
        "print(\"\\nAFTER backward\")\n",
        "for element in [a, b, c, d, e, f, g]:\n",
        "    print(element)"
      ],
      "metadata": {
        "id": "k_9ZeyYfwITr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Solve the problem with subtraction\n",
        "a = Value(data=6.0)\n",
        "b = Value(data=3.0)\n",
        "\n",
        "c = a - b\n",
        "c.backward()\n",
        "print(f\"c => {c}\")\n",
        "print(f\"a => {a}\")\n",
        "print(f\"b => {b}\")"
      ],
      "metadata": {
        "id": "0EWxClNILghB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Solve the problem with division\n",
        "a = Value(data=6.0)\n",
        "b = Value(data=3.0)\n",
        "\n",
        "c = a / b\n",
        "c.backward()\n",
        "print(f\"c => {c}\")\n",
        "print(f\"a => {a}\")\n",
        "print(f\"b => {b}\")"
      ],
      "metadata": {
        "id": "MWGGNyVuL6Ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build a Multilayer Perceptron with `micrograd`"
      ],
      "metadata": {
        "id": "wXZs-Uviq7Kl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Module"
      ],
      "metadata": {
        "id": "XXRd1NB75jy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Module(object):\n",
        "    \"\"\"\n",
        "    The parent class for all neural network modules.\n",
        "    \"\"\"\n",
        "\n",
        "    def zero_grad(self):\n",
        "        # Zero out the gradients of all parameters.\n",
        "        for p in self.parameters():\n",
        "            p.grad = 0\n",
        "\n",
        "    def parameters(self):\n",
        "        # Initialize a parameters function that all the children will\n",
        "        # override and return a list of parameters.\n",
        "        return []"
      ],
      "metadata": {
        "id": "RQC1UYJ3r1O5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Neuron"
      ],
      "metadata": {
        "id": "LdYOTBn95x8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Neuron(Module):\n",
        "    \"\"\"\n",
        "    A single neuron.\n",
        "    Parameters:\n",
        "        number_inputs (int): number of inputs\n",
        "        is_nonlinear (bool): whether to apply ReLU nonlinearity\n",
        "        name (int): the index of neuron\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, number_inputs: int, name, is_nonlinear: bool = True):\n",
        "        # Create weights for the neuron. The weights are initialized\n",
        "        # from a random uniform distribution.\n",
        "        self.weights = [Value(data=random.uniform(-1, 1)) for _ in range(number_inputs)]\n",
        "\n",
        "        # Create bias for the neuron.\n",
        "        self.bias = Value(data=0.0)\n",
        "        self.is_nonlinear = is_nonlinear\n",
        "\n",
        "        self.name = name\n",
        "\n",
        "    def __call__(self, x: List[\"Value\"]) -> \"Value\":\n",
        "        # Compute the dot product of the input and the weights. Add the\n",
        "        # bias to the dot product.\n",
        "        act = sum(\n",
        "            ((wi * xi) for wi, xi in zip(self.weights, x)),\n",
        "            self.bias\n",
        "        )\n",
        "\n",
        "        # If activation is mentioned apply ReLU to it.\n",
        "        return act.relu() if self.is_nonlinear else act\n",
        "\n",
        "    def parameters(self):\n",
        "        # Get the parameters of the neuron. The parameters of a neuron\n",
        "        # is its weights and bias.\n",
        "        return self.weights + [self.bias]\n",
        "\n",
        "    def __repr__(self):\n",
        "        # Print a better representation of the neuron.\n",
        "        return f\"Neuron {self.name}(Number={len(self.weights)}, Non-Linearity={'ReLU' if self.is_nonlinear else 'None'})\""
      ],
      "metadata": {
        "id": "aXsfxdu_q_uU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = [2.0, 3.0]\n",
        "neuron = Neuron(number_inputs=2, name=1)\n",
        "print(neuron)\n",
        "out = neuron(x)\n",
        "print(f\"Output => {out}\")"
      ],
      "metadata": {
        "id": "9oqqQ3AEt-Ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Layer"
      ],
      "metadata": {
        "id": "NZndo8YN6-lP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer(Module):\n",
        "    \"\"\"\n",
        "    A layer of neurons.\n",
        "    Parameters:\n",
        "        number_inputs (int): number of inputs\n",
        "        number_outputs (int): number of outputs\n",
        "        name (int): index of the layer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, number_inputs: int, number_outputs: int, name: int, **kwargs):\n",
        "        # A layer is a list of neurons.\n",
        "        self.neurons = [\n",
        "            Neuron(number_inputs=number_inputs, name=idx, **kwargs) for idx in range(\n",
        "                number_outputs)\n",
        "        ]\n",
        "        self.name = name\n",
        "        self.number_outputs = number_outputs\n",
        "\n",
        "    def __call__(self, x: List[\"Value\"]) -> Union[List[\"Value\"], \"Value\"]:\n",
        "        # Iterate over all the neurons and compute the output of each.\n",
        "        out = [n(x) for n in self.neurons]\n",
        "        return out if self.number_outputs != 1 else out[0]\n",
        "\n",
        "    def parameters(self):\n",
        "        # The parameters of a layer is the parameters of all the neurons.\n",
        "        return [p for n in self.neurons for p in n.parameters()]\n",
        "\n",
        "    def __repr__(self):\n",
        "        # Print a better representation of the layer.\n",
        "        layer_str = \"\\n\".join(f'    - {str(n)}' for n in self.neurons)\n",
        "        return f\"Layer {self.name} \\n{layer_str}\\n\""
      ],
      "metadata": {
        "id": "bIQ3h9sCr5OH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = [2.0, 3.0]\n",
        "layer = Layer(number_inputs=2, number_outputs=3, name=1)\n",
        "print(layer)\n",
        "out = layer(x)\n",
        "print(f\"Output => {out}\")"
      ],
      "metadata": {
        "id": "tI1I96aszsw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = [2.0, 3.0]\n",
        "layer = Layer(number_inputs=2, number_outputs=1, name=1)\n",
        "print(layer)\n",
        "out = layer(x)\n",
        "print(f\"Output => {out}\")"
      ],
      "metadata": {
        "id": "1yWby79Pn840"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Multilayer Perceptron"
      ],
      "metadata": {
        "id": "5LhwY7Dc7FXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(Module):\n",
        "    \"\"\"\n",
        "    The Multi-Layer Perceptron (MLP) class.\n",
        "    Parameters:\n",
        "        number_inputs (int): number of inputs.\n",
        "        list_number_outputs (List[int]): number of outputs in each layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, number_inputs: int, list_number_outputs: List[int]):\n",
        "        # Get the number of inputs and all the number of outputs in\n",
        "        # a single list.\n",
        "        total_size = [number_inputs] + list_number_outputs\n",
        "\n",
        "        # Build layers by connecting each layer to the previous one.\n",
        "        self.layers = [\n",
        "            # Do not use non linearity in the last layer.\n",
        "            Layer(\n",
        "                number_inputs=total_size[i],\n",
        "                number_outputs=total_size[i + 1],\n",
        "                name=i,\n",
        "                is_nonlinear=i != len(list_number_outputs) - 1\n",
        "            )\n",
        "            for i in range(len(list_number_outputs))\n",
        "        ]\n",
        "\n",
        "    def __call__(self, x: List[\"Value\"]) -> List[\"Value\"]:\n",
        "        # Iterate over the layers and compute the output of\n",
        "        # each sequentially.\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    def parameters(self):\n",
        "        # Get the parameters of the MLP\n",
        "        return [p for layer in self.layers for p in layer.parameters()]\n",
        "\n",
        "    def __repr__(self):\n",
        "        # Print a better representation of the MLP.\n",
        "        mlp_str = \"\\n\".join(f'  - {str(layer)}' for layer in self.layers)\n",
        "        return f\"MLP of \\n{mlp_str}\""
      ],
      "metadata": {
        "id": "0gNFA1pcr7XK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = [2.0, 3.0]\n",
        "mlp = MLP(number_inputs=2, list_number_outputs=[3, 3, 1])\n",
        "print(mlp)\n",
        "out = mlp(x)\n",
        "print(f\"Output => {out}\")"
      ],
      "metadata": {
        "id": "XWmEZepG22HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train the MLP"
      ],
      "metadata": {
        "id": "Bc7ppSj07fVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a dataset\n",
        "xs = [\n",
        "    [0.5, 0.5, 0.70],\n",
        "    [0.4, -0.1, 0.5],\n",
        "    [-0.2, -0.75, 1.0],\n",
        "]\n",
        "ys = [0.0, 1.0, 0.0]\n",
        "\n",
        "# Build an MLP\n",
        "mlp = MLP(number_inputs=3, list_number_outputs=[3, 3, 1])"
      ],
      "metadata": {
        "id": "oKX4T22r7hRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(mlp: \"MLP\", xs: List[List[float]]) -> List[\"Value\"]:\n",
        "    # Get the predictions upon forwarding the input data through\n",
        "    # the mlp\n",
        "    ypred = [mlp(x) for x in xs]\n",
        "    return ypred\n",
        "\n",
        "def compute_loss(ys: List[int], ypred: List[\"Value\"]) -> \"Value\":\n",
        "    # Obtain the L2 distance of the prediction and ground truths\n",
        "    loss = sum(\n",
        "        [(ygt - yout)**2 for ygt, yout in zip(ys, ypred)]\n",
        "    )\n",
        "    return loss\n",
        "\n",
        "def update_mlp(mlp: \"MLP\"):\n",
        "    # Iterate over all the layers of the MLP\n",
        "    for layer in mlp.layers:\n",
        "        # Iterate over all the neurons of each layer\n",
        "        for neuron in layer.neurons:\n",
        "            # Iterate over all the weights of each neuron\n",
        "            for weight in neuron.weights:\n",
        "                # Update the data of the weight with the \n",
        "                # gradient information.\n",
        "                weight.data -= (1e-2 * weight.grad)\n",
        "            # Update the data of the bias with the \n",
        "            # gradient information.\n",
        "            neuron.bias.data -= (1e-2 * neuron.bias.grad)"
      ],
      "metadata": {
        "id": "nmqtvs6OdxwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the epochs for which we want to run the training process.\n",
        "epochs = 50\n",
        "\n",
        "# Define a loss list to help log the loss.\n",
        "loss_list = []\n",
        "\n",
        "# Iterate each epoch and train the model.\n",
        "for idx in range(epochs):\n",
        "    # Step 1: Forward the inputs to the mlp and get the predictions\n",
        "    ypred = forward(mlp, xs)\n",
        "    # Step 2: Compute Loss between the predictions and the ground truths\n",
        "    loss = compute_loss(ys, ypred)\n",
        "    # Step 3: Ground the gradients. These accumulate which is not desired.\n",
        "    mlp.zero_grad()\n",
        "    # Step 4: Backpropagate the gradients through the entire architecture\n",
        "    loss.backward()\n",
        "    # Step 5: Update the mlp\n",
        "    update_mlp(mlp)\n",
        "    # Step 6: Log the loss\n",
        "    loss_list.append(loss.data)\n",
        "    print(f\"Epoch {idx}: Loss {loss.data: 0.2f}\")"
      ],
      "metadata": {
        "id": "dZ_uc5Pvlx_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss\n",
        "plt.plot(loss_list)\n",
        "plt.grid()\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TASagzfogd7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "pred = mlp(xs[0])\n",
        "ygt = ys[0]\n",
        "\n",
        "print(f\"Prediction => {pred.data: 0.2f}\")\n",
        "print(f\"Ground Truth => {ygt: 0.2f}\")"
      ],
      "metadata": {
        "id": "5-ncsSSZQDXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ogkNauArL6u"
      },
      "source": [
        "For a detailed walkthrough of the concepts and code, be sure to refer to the full tutorial, [*Automatic Differentiation Part 2: Implementation Using Micrograd*](https://pyimg.co/ra6ow) published on 2022-12-26."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sef8alx4cGYc"
      },
      "source": [
        "# Code License Agreement\n",
        "```\n",
        "Copyright (c) 2022 PyImageSearch.com\n",
        "\n",
        "SIMPLE VERSION\n",
        "Feel free to use this code for your own projects, whether they are\n",
        "purely educational, for fun, or for profit. THE EXCEPTION BEING if\n",
        "you are developing a course, book, or other educational product.\n",
        "Under *NO CIRCUMSTANCE* may you use this code for your own paid\n",
        "educational or self-promotional ventures without written consent\n",
        "from OptiReto, LLC and PyImageSearch.com.\n",
        "\n",
        "LONGER, FORMAL VERSION\n",
        "Permission is hereby granted, free of charge, to any person obtaining\n",
        "a copy of this software and associated documentation files\n",
        "(the \"Software\"), to deal in the Software without restriction,\n",
        "including without limitation the rights to use, copy, modify, merge,\n",
        "publish, distribute, sublicense, and/or sell copies of the Software,\n",
        "and to permit persons to whom the Software is furnished to do so,\n",
        "subject to the following conditions:\n",
        "The above copyright notice and this permission notice shall be\n",
        "included in all copies or substantial portions of the Software.\n",
        "Notwithstanding the foregoing, you may not use, copy, modify, merge,\n",
        "publish, distribute, sublicense, create a derivative work, and/or\n",
        "sell copies of the Software in any work that is designed, intended,\n",
        "or marketed for pedagogical or instructional purposes related to\n",
        "programming, coding, application development, or information\n",
        "technology. Permission for such use, copying, modification, and\n",
        "merger, publication, distribution, sub-licensing, creation of\n",
        "derivative works, or sale is expressly withheld.\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n",
        "EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES\n",
        "OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n",
        "NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n",
        "BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n",
        "ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
        "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\n",
        "```"
      ]
    }
  ]
}