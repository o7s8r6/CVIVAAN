{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/o7s8r6/CVIVAAN/blob/main/nerf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEYvcfHRB8dw"
      },
      "source": [
        "![pyimagesearch_university_logo.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4RpK9pawQzP"
      },
      "source": [
        "# Computer Graphics and Deep Learning with NeRF using TensorFlow and Keras\n",
        "### by [PyImageSearch.com](http://www.pyimagesearch.com)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ntZ1AkXZIxY"
      },
      "source": [
        "## Welcome to **[PyImageSearch University](https://pyimg.co/university)** Jupyter Notebooks!\n",
        "\n",
        "This notebook is associated with the Computer Graphics and Deep Learning with NeRF using TensorFlow and Keras Series.\n",
        "- [Part 1](https://www.pyimagesearch.com/2021/11/10/computer-graphics-and-deep-learning-with-nerf-using-tensorflow-and-keras-part-1/) published on 2021-11-10.\n",
        "- [Part 2](https://www.pyimagesearch.com/2021/11/17/computer-graphics-and-deep-learning-with-nerf-using-tensorflow-and-keras-part-2/) published on 2021-11-17\n",
        "- [Part 3](https://www.pyimagesearch.com/2021/11/24/computer-graphics-and-deep-learning-with-nerf-using-tensorflow-and-keras-part-3/) published on 2021-11-24\n",
        "\n",
        "Only the code for the blog post is here. Most codeblocks have a 1:1 relationship with what you find in the blog post with two exceptions: (1) Python classes are not separate files as they are typically organized with PyImageSearch projects, and (2) Command Line Argument parsing is replaced with an `args` dictionary that you can manipulate as needed.\n",
        "\n",
        "We recommend that you execute (press ▶️) the code block-by-block, as-is, before adjusting parameters and `args` inputs. Once you've verified that the code is working, you are welcome to hack with it and learn from manipulating inputs, settings, and parameters. For more information on using Jupyter and Colab, please refer to these resources:\n",
        "\n",
        "*   [Jupyter Notebook User Interface](https://jupyter-notebook.readthedocs.io/en/stable/notebook.html#notebook-user-interface)\n",
        "*   [Overview of Google Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "As a reminder, these PyImageSearch University Jupyter Notebooks are not for sharing; please refer to the **Copyright** directly below and **Code License Agreement** in the last cell of this notebook. \n",
        "\n",
        "Happy hacking!\n",
        "\n",
        "*Adrian*\n",
        "\n",
        "<hr>\n",
        "\n",
        "***Copyright:*** *The contents of this Jupyter Notebook, unless otherwise indicated, are Copyright 2021 Adrian Rosebrock, PyimageSearch.com. All rights reserved. Content like this is made possible by the time invested by the authors. If you received this Jupyter Notebook and did not purchase it, please consider making future content possible by joining PyImageSearch University at https://pyimg.co/university today.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFhAzQB3aNMa"
      },
      "source": [
        "### Download the code zip file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y0LG1EuaRlB"
      },
      "source": [
        "!wget https://pyimagesearch-code-downloads.s3-us-west-2.amazonaws.com/neural-radiance-fields/neural-radiance-fields.zip\n",
        "!unzip -qq neural-radiance-fields.zip\n",
        "%cd neural-radiance-fields"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pjsTrZ6NLJQ"
      },
      "source": [
        "### Download the dataset\n",
        "\n",
        "⚠️ Before running any of the code below please download the dataset from [this Gdrive link](https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1). The dataset is open sourced by the authors and the link can be found in the [official NeRF repository](https://github.com/bmild/nerf).\n",
        "\n",
        "Steps to download the dataset:\n",
        "\n",
        "- Visit this [GDrive link](https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1)\n",
        "- Now go into the `nerf_synthetic` folder.\n",
        "- Right click on any one of the folder (eg. `lego`) and click on **download**.\n",
        "- Clicking on the **downlaod** button will get you a zip file containing the entire dataset that you chose (eg. `lego.zip`)\n",
        "- After downloading a dataset (eg. `lego.zip`) use the following commands to expand the zip.\n",
        "```\n",
        "$ unzip -qq <DATASET_NAME>.zip\n",
        "$ mv <DATASET_NAME> dataset\n",
        "```\n",
        "In place of `<DATASET_NAME>` use the name of the dataset that you have downloaded.\n",
        "\n",
        "After completing the dataset download, proceed to the later sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SgTVT3HagGZ"
      },
      "source": [
        "## Blog Post Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcrOk6pURp50"
      },
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJaCNlDDRz6d"
      },
      "source": [
        "# import the necessary packages\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import array_to_img\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.image import convert_image_dtype\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.metrics import Mean\n",
        "from tensorflow.image import decode_jpeg\n",
        "from tensorflow.data import AUTOTUNE\n",
        "from tensorflow.image import resize\n",
        "from tensorflow.io import read_file\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras import Model\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "from tensorflow import reshape\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import imageio\n",
        "import json\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCOp7ZYr1RuW"
      },
      "source": [
        "### Creating our configuration file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjFsu0AX1UEI"
      },
      "source": [
        "class Config:\n",
        "    # define the dataset path\n",
        "    DATASET_PATH = \"dataset\"\n",
        "\n",
        "    # define the json paths\n",
        "    TRAIN_JSON = os.path.join(DATASET_PATH, \"transforms_train.json\")\n",
        "    VAL_JSON = os.path.join(DATASET_PATH, \"transforms_val.json\")\n",
        "    TEST_JSON = os.path.join(DATASET_PATH, \"transforms_test.json\")\n",
        "\n",
        "    # define TensorFlow AUTOTUNE\n",
        "    AUTO = AUTOTUNE\n",
        "\n",
        "    # define image dimensions\n",
        "    IMAGE_WIDTH = 50\n",
        "    IMAGE_HEIGHT = 50\n",
        "\n",
        "    # define the number of samples for coarse and fine model\n",
        "    N_C = 16\n",
        "    N_F = 32\n",
        "\n",
        "    # define the dimension for positional encoding\n",
        "    L_XYZ = 8\n",
        "    L_DIR = 4\n",
        "\n",
        "    # define the near and far bounding values of the 3D scene\n",
        "    NEAR = 2.0\n",
        "    FAR = 6.0\n",
        "\n",
        "    # define the batch size\n",
        "    BATCH_SIZE = 1\n",
        "\n",
        "    # define the number of dense units\n",
        "    DENSE_UNITS = 64\n",
        "\n",
        "    # define the skip layer\n",
        "    SKIP_LAYER = 4\n",
        "\n",
        "    # define the model fit parameters\n",
        "    STEPS_PER_EPOCH = 50\n",
        "    VALIDATION_STEPS = 5\n",
        "    EPOCHS = 20\n",
        "\n",
        "    # define a output image path\n",
        "    OUTPUT_PATH = \"output\"\n",
        "    IMAGE_PATH = os.path.join(OUTPUT_PATH, \"images\")\n",
        "    VIDEO_PATH = os.path.join(OUTPUT_PATH, \"videos\")\n",
        "\n",
        "    # define the parameters of the rendered video\n",
        "    SAMPLE_THETA_POINTS = 90\n",
        "    FPS = 30\n",
        "    QUALITY = 7\n",
        "    MACRO_BLOCK_SIZE = None\n",
        "\n",
        "    # define the inference video path\n",
        "    OUTPUT_VIDEO_PATH = os.path.join(VIDEO_PATH, \"ouptut.mp4\")\n",
        "\n",
        "    # define coarse and fine model paths\n",
        "    COARSE_PATH = os.path.join(OUTPUT_PATH, \"coarse\")\n",
        "    FINE_PATH = os.path.join(OUTPUT_PATH, \"fine\")\n",
        "\n",
        "config = Config()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa4CiJM1R3Cp"
      },
      "source": [
        "### Utiliy functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fUuRJ3aSUSV"
      },
      "source": [
        "def get_focal_from_fov(fieldOfView, width):\n",
        "\t# calculate the focal length of the camera from the field of view\n",
        "\tfocalLength = 0.5 * width / tf.tan(0.5 * fieldOfView)\n",
        "\t\n",
        "\t# return the focal length\n",
        "\treturn focalLength"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfETRlttSXH2"
      },
      "source": [
        "def render_image_depth(rgb, sigma, tVals):\n",
        "\t# squeeze the last dimension of sigma\n",
        "\tsigma = sigma[..., 0]\n",
        "\n",
        "\t# calculate the delta between adjacent tVals\n",
        "\tdelta = tVals[..., 1:] - tVals[..., :-1]\n",
        "\tdeltaShape = [config.BATCH_SIZE, config.IMAGE_HEIGHT, config.IMAGE_WIDTH, 1]\n",
        "\tdelta = tf.concat(\n",
        "\t\t[delta, tf.broadcast_to([1e10], shape=deltaShape)], axis=-1)\n",
        "\n",
        "\t# calculate alpha from sigma and delta values\n",
        "\talpha = 1.0 - tf.exp(-sigma * delta)\n",
        "\n",
        "\t# calculate the exponential term for easir calculations\n",
        "\texpTerm = 1.0 - alpha\n",
        "\tepsilon = 1e-10\n",
        "\n",
        "\t# calculate the transmittance and weights of the ray points\n",
        "\ttransmittance = tf.math.cumprod(expTerm + epsilon, axis=-1,\n",
        "\t\texclusive=True)\n",
        "\tweights = alpha * transmittance\n",
        "\t\n",
        "\t# build the image and depth map from the points of the rays\n",
        "\timage = tf.reduce_sum(weights[..., None] * rgb, axis=-2)\n",
        "\tdepth = tf.reduce_sum(weights * tVals, axis=-1)\n",
        "\t\n",
        "\t# return rgb, depth map and weights\n",
        "\treturn (image, depth, weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZPZXyPMSZb6"
      },
      "source": [
        "def sample_pdf(tValsMid, weights, nF):\n",
        "\t# add a small value to the weights to prevent it form nan\n",
        "\tweights += 1e-5\n",
        "\n",
        "\t# normalize the weights to get the pdf\n",
        "\tpdf = weights / tf.reduce_sum(weights, axis=-1, keepdims=True)\n",
        "\n",
        "\t# from pdf to cdf transformation\n",
        "\tcdf = tf.cumsum(pdf, axis=-1)\n",
        "\n",
        "\t# start the cdf with 0s\n",
        "\tcdf = tf.concat([tf.zeros_like(cdf[..., :1]), cdf], axis=-1)\n",
        "\n",
        "\t# get the sample points\n",
        "\tuShape = [config.BATCH_SIZE, config.IMAGE_HEIGHT, config.IMAGE_WIDTH, nF]\n",
        "\tu = tf.random.uniform(shape=uShape)\n",
        "\n",
        "\t# get the indices of the points of u when u is inserted into cdf in a\n",
        "\t# sorted manner\n",
        "\tindices = tf.searchsorted(cdf, u, side=\"right\")\n",
        "\n",
        "\t# define the boundaries\n",
        "\tbelow = tf.maximum(0, indices-1)\n",
        "\tabove = tf.minimum(cdf.shape[-1]-1, indices)\n",
        "\tindicesG = tf.stack([below, above], axis=-1)\n",
        "\t\n",
        "\t# gather the cdf according to the indices\n",
        "\tcdfG = tf.gather(cdf, indicesG, axis=-1,\n",
        "\t\tbatch_dims=len(indicesG.shape)-2)\n",
        "\t\n",
        "\t# gather the tVals according to the indices\n",
        "\ttValsMidG = tf.gather(tValsMid, indicesG, axis=-1,\n",
        "\t\tbatch_dims=len(indicesG.shape)-2)\n",
        "\n",
        "\t# create the samples by inverting the cdf\n",
        "\tdenom = cdfG[..., 1] - cdfG[..., 0]\n",
        "\tdenom = tf.where(denom < 1e-5, tf.ones_like(denom), denom)\n",
        "\tt = (u - cdfG[..., 0]) / denom\n",
        "\tsamples = (tValsMidG[..., 0] + t * \n",
        "\t\t(tValsMidG[..., 1] - tValsMidG[..., 0]))\n",
        "\t\n",
        "\t# return the samples\n",
        "\treturn samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHOCDSkhR5iD"
      },
      "source": [
        "def get_translation_t(t):\n",
        "\t# build the translation matrix and convert it to a tensor\n",
        "\tmatrix = [\n",
        "\t\t[1, 0, 0, 0],\n",
        "\t\t[0, 1, 0, 0],\n",
        "\t\t[0, 0, 1, t],\n",
        "\t\t[0, 0, 0, 1],\n",
        "\t]\n",
        "\tmatrix = tf.convert_to_tensor(matrix, dtype=tf.float32)\n",
        "\t\n",
        "\t# return the translation matrix\n",
        "\treturn matrix\n",
        "\n",
        "def get_rotation_phi(phi):\n",
        "\t# build the roation matrix and convert it to a tensor\n",
        "\tmatrix = [\n",
        "\t\t[1, 0, 0, 0],\n",
        "\t\t[0, tf.cos(phi), -tf.sin(phi), 0],\n",
        "\t\t[0, tf.sin(phi), tf.cos(phi), 0],\n",
        "\t\t[0, 0, 0, 1],\n",
        "\t]\n",
        "\tmatrix = tf.convert_to_tensor(matrix, dtype=tf.float32)\n",
        "\n",
        "\t# return the rotation matrix\n",
        "\treturn matrix\n",
        "\n",
        "def get_rotation_theta(theta):\n",
        "\t# build the roation matrix and convert it to a tensor\n",
        "\tmatrix = [\n",
        "\t\t[tf.cos(theta), 0, -tf.sin(theta), 0],\n",
        "\t\t[0, 1, 0, 0],\n",
        "\t\t[tf.sin(theta), 0, tf.cos(theta), 0],\n",
        "\t\t[0, 0, 0, 1],\n",
        "\t]\n",
        "\tmatrix = tf.convert_to_tensor(matrix, dtype=tf.float32)\n",
        "\n",
        "\t# return the rotation matrix\n",
        "\treturn matrix\n",
        "\n",
        "def pose_spherical(theta, phi, t):\n",
        "\t# compute the camera2world matrix from the give theta, phi and t\n",
        "\tc2w = get_translation_t(t)\n",
        "\tc2w = get_rotation_phi(phi / 180.0 * np.pi) @ c2w\n",
        "\tc2w = get_rotation_theta(theta / 180.0 * np.pi) @ c2w\n",
        "\tc2w = np.array([[-1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 0, 1]]) @ c2w\n",
        "\t\n",
        "\t# return the camera2world matrix\n",
        "\treturn c2w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32ORy_RNI_X7"
      },
      "source": [
        "### Data utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUhLseRCQOPE"
      },
      "source": [
        "def read_json(jsonPath):\n",
        "\t# open the json file\n",
        "\twith open(jsonPath, \"r\") as fp:\n",
        "\t\t# read the json data\n",
        "\t\tdata = json.load(fp)\n",
        "\t\n",
        "\t# return the data\n",
        "\treturn data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjC-KnDHQQcg"
      },
      "source": [
        "def get_image_c2w(jsonData, datasetPath):\n",
        "\t# define a list to store the image paths\n",
        "\timagePaths = []\n",
        "\t\n",
        "\t# define a list to store the camera2world matrices\n",
        "\tc2ws = []\n",
        "\n",
        "\t# iterate over each frame of the data\n",
        "\tfor frame in jsonData[\"frames\"]:\n",
        "\t\t# grab the image file name\n",
        "\t\timagePath = frame[\"file_path\"]\n",
        "\t\timagePath = imagePath.replace(\".\", datasetPath)\n",
        "\t\timagePaths.append(f\"{imagePath}.png\")\n",
        "\n",
        "\t\t# grab the camera2world matrix\n",
        "\t\tc2ws.append(frame[\"transform_matrix\"])\n",
        "\t\n",
        "\t# return the image file names and the camera2world matrices\n",
        "\treturn (imagePaths, c2ws)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K53vLqTYQTuT"
      },
      "source": [
        "class GetImages:\n",
        "\tdef __init__(self, imageWidth, imageHeight):\n",
        "\t\t# define the image width and height\n",
        "\t\tself.imageWidth = imageWidth\n",
        "\t\tself.imageHeight = imageHeight\n",
        "\n",
        "\tdef __call__(self, imagePath):\n",
        "\t\t# read the image file\n",
        "\t\timage = read_file(imagePath)\n",
        "\n",
        "\t\t# decode the image string\n",
        "\t\timage = decode_jpeg(image, 3)\n",
        "\n",
        "\t\t# convert the image dtype from uint8 to float32\n",
        "\t\timage = convert_image_dtype(image, dtype=tf.float32)\n",
        "\n",
        "\t\t# resize the image to the height and width in cofig\n",
        "\t\timage = resize(image, (self.imageWidth, self.imageHeight))\n",
        "\t\timage = reshape(image, (self.imageWidth, self.imageHeight, 3))\n",
        "\n",
        "\t\t# return the image\n",
        "\t\treturn image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fF3Ep2wHjH9"
      },
      "source": [
        "class GetRays:\n",
        "\tdef __init__(self, focalLength, imageWidth, imageHeight, near, \n",
        "\t\tfar, nC):\n",
        "\t\t# define the focal length, image width, and image height\n",
        "\t\tself.focalLength = focalLength\n",
        "\t\tself.imageWidth = imageWidth\n",
        "\t\tself.imageHeight = imageHeight\n",
        "\n",
        "\t\t# define the near and far bounding values\n",
        "\t\tself.near = near\n",
        "\t\tself.far = far\n",
        "\n",
        "\t\t# define the number of samples for coarse model\n",
        "\t\tself.nC = nC\n",
        "\n",
        "\tdef __call__(self, camera2world):\n",
        "\t\t# create a meshgrid of image dimensions\n",
        "\t\t(x, y) = tf.meshgrid(\n",
        "\t\t\ttf.range(self.imageWidth, dtype=tf.float32),\n",
        "\t\t\ttf.range(self.imageHeight, dtype=tf.float32),\n",
        "\t\t\tindexing=\"xy\",\n",
        "\t\t)\n",
        "\n",
        "\t\t# define the camera coordinates\n",
        "\t\txCamera = (x - self.imageWidth * 0.5) / self.focalLength\n",
        "\t\tyCamera = (y - self.imageHeight * 0.5) / self.focalLength\n",
        "\n",
        "\t\t# define the camera vector\n",
        "\t\txCyCzC = tf.stack([xCamera, -yCamera, -tf.ones_like(x)],\n",
        "\t\t\taxis=-1)\n",
        "\n",
        "\t\t# slice the camera2world matrix to obtain the roataion and\n",
        "\t\t# translation matrix\n",
        "\t\trotation = camera2world[:3, :3]\n",
        "\t\ttranslation = camera2world[:3, -1]\n",
        "\n",
        "\t\t# expand the camera coordinates to \n",
        "\t\txCyCzC = xCyCzC[..., None, :]\n",
        "\t\t\n",
        "\t\t# get the world coordinates\n",
        "\t\txWyWzW = xCyCzC * rotation\n",
        "\t\t\n",
        "\t\t# calculate the direciton vector of the ray\n",
        "\t\trayD = tf.reduce_sum(xWyWzW, axis=-1)\n",
        "\t\trayD = rayD / tf.norm(rayD, axis=-1, keepdims=True)\n",
        "\n",
        "\t\t# calculate the origin vector of the ray\n",
        "\t\trayO = tf.broadcast_to(translation, tf.shape(rayD))\n",
        "\n",
        "\t\t# get the sample points from the ray\n",
        "\t\ttVals = tf.linspace(self.near, self.far, self.nC)\n",
        "\t\tnoiseShape = list(rayO.shape[:-1]) + [self.nC]\n",
        "\t\tnoise = (tf.random.uniform(shape=noiseShape) * \n",
        "\t\t\t(self.far - self.near) / self.nC)\n",
        "\t\ttVals = tVals + noise\n",
        "\n",
        "\t\t# return ray origin, direction, and the sample points\n",
        "\t\treturn (rayO, rayD, tVals)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5aKHcu9JCbN"
      },
      "source": [
        "### Positional Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37bskRA0Q0M5"
      },
      "source": [
        "def encoder_fn(p, L):\n",
        "\t# build the list of positional encodings\n",
        "\tgamma = [p]\n",
        "\n",
        "\t# iterate over the number of dimensions in time\n",
        "\tfor i in range(L):\n",
        "\t\t# insert sine and cosine of the product of current dimension\n",
        "\t\t# and the position vector\n",
        "\t\tgamma.append(tf.sin((2.0 ** i) * p))\n",
        "\t\tgamma.append(tf.cos((2.0 ** i) * p))\n",
        "\t\n",
        "\t# concatenate the positional encodings into a positional vector\n",
        "\tgamma = tf.concat(gamma, axis=-1)\n",
        "\n",
        "\t# return the positional encoding vector\n",
        "\treturn gamma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX5PAA1bQ5jn"
      },
      "source": [
        "### NeRF Multilayer Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wk3PWJmXQ8Qu"
      },
      "source": [
        "def get_model(lxyz, lDir, batchSize, denseUnits, skipLayer):\n",
        "\t# build input layer for rays\n",
        "\trayInput = Input(shape=(None, None, None, 2 * 3 * lxyz + 3),\n",
        "\t\tbatch_size=batchSize)\n",
        "\t\n",
        "\t# build input layer for direction of the rays\n",
        "\tdirInput = Input(shape=(None, None, None, 2 * 3 * lDir + 3),\n",
        "\t\tbatch_size=batchSize)\n",
        "\t\n",
        "\t# creating an input for the MLP\n",
        "\tx = rayInput\n",
        "\tfor i in range(8):\n",
        "\t\t# build a dense layer\n",
        "\t\tx = Dense(units=denseUnits, activation=\"relu\")(x)\n",
        "\n",
        "\t\t# check if we have to include residual connection\n",
        "\t\tif i % skipLayer == 0 and i > 0:\n",
        "\t\t\t# inject the residual connection\n",
        "\t\t\tx = concatenate([x, rayInput], axis=-1)\n",
        "\t\n",
        "\t# get the sigma value\n",
        "\tsigma = Dense(units=1, activation=\"relu\")(x)\n",
        "\n",
        "\t# create the feature vector\n",
        "\tfeature = Dense(units=denseUnits)(x)\n",
        "\n",
        "\t# concatenate the feature vector with the direction input and put\n",
        "\t# it through a dense layer\n",
        "\tfeature = concatenate([feature, dirInput], axis=-1)\n",
        "\tx = Dense(units=denseUnits//2, activation=\"relu\")(feature)\n",
        "\n",
        "\t# get the rgb value\n",
        "\trgb = Dense(units=3, activation=\"sigmoid\")(x)\n",
        "\n",
        "\t# create the nerf model\n",
        "\tnerfModel = Model(inputs=[rayInput, dirInput],\n",
        "\t\toutputs=[rgb, sigma])\n",
        "\t\n",
        "\t# return the nerf model\n",
        "\treturn nerfModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvybIvHZRMz0"
      },
      "source": [
        "### Custom NeRF Trainer model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4DvTtCqRMjJ"
      },
      "source": [
        "class Nerf_Trainer(tf.keras.Model):\n",
        "\tdef __init__(self, coarseModel, fineModel, lxyz, lDir, \n",
        "\t\tencoderFn, renderImageDepth, samplePdf, nF):\n",
        "\t\tsuper().__init__()\n",
        "\t\t# define the coarse model and fine model\n",
        "\t\tself.coarseModel = coarseModel\n",
        "\t\tself.fineModel = fineModel\n",
        "\n",
        "\t\t# define the dimensions for positional encoding for spatial\n",
        "\t\t# coordinates and direction\n",
        "\t\tself.lxyz = lxyz\n",
        "\t\tself.lDir = lDir\n",
        "\n",
        "\t\t# define the positional encoder\n",
        "\t\tself.encoderFn = encoderFn\n",
        "\n",
        "\t\t# define the volume rendering function\n",
        "\t\tself.renderImageDepth = renderImageDepth\n",
        "\n",
        "\t\t# define the hierarchical sampling function and the number of\n",
        "\t\t# samples for the fine model\n",
        "\t\tself.samplePdf = samplePdf\n",
        "\t\tself.nF = nF\n",
        "\n",
        "\tdef compile(self, optimizerCoarse, optimizerFine, lossFn):\n",
        "\t\tsuper().compile()\n",
        "\t\t# define the optimizer for the coarse and fine model\n",
        "\t\tself.optimizerCoarse = optimizerCoarse\n",
        "\t\tself.optimizerFine = optimizerFine\n",
        "\n",
        "\t\t# define the photometric loss function\n",
        "\t\tself.lossFn = lossFn\n",
        "\n",
        "\t\t# define the loss and psnr tracker\n",
        "\t\tself.lossTracker = Mean(name=\"loss\")\n",
        "\t\tself.psnrMetric = Mean(name=\"psnr\")\n",
        "\n",
        "\tdef train_step(self, inputs):\n",
        "\t\t# get the images and the rays\n",
        "\t\t(elements, images) = inputs\n",
        "\t\t(raysOriCoarse, raysDirCoarse, tValsCoarse) = elements\n",
        "\n",
        "\t\t# generate the coarse rays\n",
        "\t\traysCoarse = (raysOriCoarse[..., None, :] + \n",
        "\t\t\t(raysDirCoarse[..., None, :] * tValsCoarse[..., None]))\n",
        "\n",
        "\t\t# positional encode the rays and dirs\n",
        "\t\traysCoarse = self.encoderFn(raysCoarse, self.lxyz)\n",
        "\t\tdirCoarseShape = tf.shape(raysCoarse[..., :3])\n",
        "\t\tdirsCoarse = tf.broadcast_to(raysDirCoarse[..., None, :],\n",
        "\t\t\tshape=dirCoarseShape)\n",
        "\t\tdirsCoarse = self.encoderFn(dirsCoarse, self.lDir)\n",
        "\n",
        "\t\t# keep track of our gradients\n",
        "\t\twith tf.GradientTape() as coarseTape:\n",
        "\t\t\t# compute the predictions from the coarse model\n",
        "\t\t\t(rgbCoarse, sigmaCoarse) = self.coarseModel([raysCoarse, \n",
        "\t\t\t\tdirsCoarse])\n",
        "\t\t\t\n",
        "\t\t\t# render the image from the predicitons\n",
        "\t\t\trenderCoarse = self.renderImageDepth(rgb=rgbCoarse,\n",
        "\t\t\t\tsigma=sigmaCoarse, tVals=tValsCoarse)\n",
        "\t\t\t(imagesCoarse, _, weightsCoarse) = renderCoarse\n",
        "\n",
        "\t\t\t# compute the photometric loss\n",
        "\t\t\tlossCoarse = self.lossFn(images, imagesCoarse)\n",
        "\n",
        "\t\t# compute the middle values of t vals\n",
        "\t\ttValsCoarseMid = (0.5 * \n",
        "\t\t\t(tValsCoarse[..., 1:] + tValsCoarse[..., :-1]))\n",
        "\n",
        "\t\t# apply hierarchical sampling and get the t vals for the fine\n",
        "\t\t# model\n",
        "\t\ttValsFine = self.samplePdf(tValsMid=tValsCoarseMid,\n",
        "\t\t\tweights=weightsCoarse, nF=self.nF)\n",
        "\t\ttValsFine = tf.sort(\n",
        "\t\t\ttf.concat([tValsCoarse, tValsFine], axis=-1), axis=-1)\n",
        "\n",
        "\t\t# build the fine rays and positional encode it\n",
        "\t\traysFine = (raysOriCoarse[..., None, :] + \n",
        "\t\t\t(raysDirCoarse[..., None, :] * tValsFine[..., None]))\n",
        "\t\traysFine = self.encoderFn(raysFine, self.lxyz)\n",
        "\t\t\n",
        "\t\t# build the fine direcitons and positional encode it\n",
        "\t\tdirsFineShape = tf.shape(raysFine[..., :3])\n",
        "\t\tdirsFine = tf.broadcast_to(raysDirCoarse[..., None, :],\n",
        "\t\t\tshape=dirsFineShape)\n",
        "\t\tdirsFine = self.encoderFn(dirsFine, self.lDir)\n",
        "\n",
        "\t\t# keep track of our gradients\n",
        "\t\twith tf.GradientTape() as fineTape:\n",
        "\t\t\t# compute the predictions from the fine model\n",
        "\t\t\trgbFine, sigmaFine = self.fineModel([raysFine, dirsFine])\n",
        "\t\t\t\n",
        "\t\t\t# render the image from the predicitons\n",
        "\t\t\trenderFine = self.renderImageDepth(rgb=rgbFine,\n",
        "\t\t\t\tsigma=sigmaFine, tVals=tValsFine)\n",
        "\t\t\t(imageFine, _, _) = renderFine\n",
        "\n",
        "\t\t\t# compute the photometric loss\n",
        "\t\t\tlossFine = self.lossFn(images, imageFine)\n",
        "\n",
        "\t\t# get the trainable variables from the coarse model and\n",
        "\t\t# apply back propagation\n",
        "\t\ttvCoarse = self.coarseModel.trainable_variables\n",
        "\t\tgradsCoarse = coarseTape.gradient(lossCoarse, tvCoarse)\n",
        "\t\tself.optimizerCoarse.apply_gradients(zip(gradsCoarse, \n",
        "\t\t\ttvCoarse))\n",
        "\n",
        "\t\t# get the trainable variables from the coarse model and\n",
        "\t\t# apply back propagation\n",
        "\t\ttvFine = self.fineModel.trainable_variables\n",
        "\t\tgradsFine = fineTape.gradient(lossFine, tvFine)\n",
        "\t\tself.optimizerFine.apply_gradients(zip(gradsFine, tvFine))\n",
        "\t\tpsnr = tf.image.psnr(images, imageFine, max_val=1.0)\n",
        "\n",
        "\t\t# compute the loss and psnr metrics\n",
        "\t\tself.lossTracker.update_state(lossFine)\n",
        "\t\tself.psnrMetric.update_state(psnr)\n",
        "\n",
        "\t\t# return the loss and psnr metrics\n",
        "\t\treturn {\"loss\": self.lossTracker.result(),\n",
        "\t\t\t\"psnr\": self.psnrMetric.result()}\n",
        "\n",
        "\tdef test_step(self, inputs):\n",
        "\t\t# get the images and the rays\n",
        "\t\t(elements, images) = inputs\n",
        "\t\t(raysOriCoarse, raysDirCoarse, tValsCoarse) = elements\n",
        "\n",
        "\t\t# generate the coarse rays\n",
        "\t\traysCoarse = (raysOriCoarse[..., None, :] + \n",
        "\t\t\t(raysDirCoarse[..., None, :] * tValsCoarse[..., None]))\n",
        "\n",
        "\t\t# positional encode the rays and dirs\n",
        "\t\traysCoarse = self.encoderFn(raysCoarse, self.lxyz)\n",
        "\t\tdirCoarseShape = tf.shape(raysCoarse[..., :3])\n",
        "\t\tdirsCoarse = tf.broadcast_to(raysDirCoarse[..., None, :],\n",
        "\t\t\tshape=dirCoarseShape)\n",
        "\t\tdirsCoarse = self.encoderFn(dirsCoarse, self.lDir)\n",
        "\n",
        "\t\t# compute the predictions from the coarse model\n",
        "\t\t(rgbCoarse, sigmaCoarse) = self.coarseModel([raysCoarse,\n",
        "\t\t\tdirsCoarse])\n",
        "\t\t\n",
        "\t\t# render the image from the predicitons\n",
        "\t\trenderCoarse = self.renderImageDepth(rgb=rgbCoarse,\n",
        "\t\t\tsigma=sigmaCoarse, tVals=tValsCoarse)\n",
        "\t\t(_, _, weightsCoarse) = renderCoarse\n",
        "\n",
        "\t\t# compute the middle values of t vals\n",
        "\t\ttValsCoarseMid = (0.5 * \n",
        "\t\t\t(tValsCoarse[..., 1:] + tValsCoarse[..., :-1]))\n",
        "\n",
        "\t\t# apply hierarchical sampling and get the t vals for the fine\n",
        "\t\t# model\n",
        "\t\ttValsFine = self.samplePdf(tValsMid=tValsCoarseMid,\n",
        "\t\t\tweights=weightsCoarse, nF=self.nF)\n",
        "\t\ttValsFine = tf.sort(\n",
        "\t\t\ttf.concat([tValsCoarse, tValsFine], axis=-1), axis=-1)\n",
        "\n",
        "\t\t# build the fine rays and positional encode it\n",
        "\t\traysFine = (raysOriCoarse[..., None, :] + \n",
        "\t\t\t(raysDirCoarse[..., None, :] * tValsFine[..., None]))\n",
        "\t\traysFine = self.encoderFn(raysFine, self.lxyz)\n",
        "\t\t\n",
        "\t\t# build the fine direcitons and positional encode it\n",
        "\t\tdirsFineShape = tf.shape(raysFine[..., :3])\n",
        "\t\tdirsFine = tf.broadcast_to(raysDirCoarse[..., None, :],\n",
        "\t\t\tshape=dirsFineShape)\n",
        "\t\tdirsFine = self.encoderFn(dirsFine, self.lDir)\n",
        "\n",
        "\t\t# compute the predictions from the fine model\n",
        "\t\trgbFine, sigmaFine = self.fineModel([raysFine, dirsFine])\n",
        "\t\t\n",
        "\t\t# render the image from the predicitons\n",
        "\t\trenderFine = self.renderImageDepth(rgb=rgbFine,\n",
        "\t\t\tsigma=sigmaFine, tVals=tValsFine)\n",
        "\t\t(imageFine, _, _) = renderFine\n",
        "\n",
        "\t\t# compute the photometric loss and psnr\n",
        "\t\tlossFine = self.lossFn(images, imageFine)\n",
        "\t\tpsnr = tf.image.psnr(images, imageFine, max_val=1.0)\n",
        "\n",
        "\t\t# compute the loss and psnr metrics\n",
        "\t\tself.lossTracker.update_state(lossFine)\n",
        "\t\tself.psnrMetric.update_state(psnr)\n",
        "\n",
        "\t\t# return the loss and psnr metrics\n",
        "\t\treturn {\"loss\": self.lossTracker.result(),\n",
        "\t\t\t\"psnr\": self.psnrMetric.result()}\n",
        "\n",
        "\t@property\n",
        "\tdef metrics(self):\n",
        "\t\t# return the loss and psnr tracker\n",
        "\t\treturn [self.lossTracker, self.psnrMetric]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f21yMU8lRZub"
      },
      "source": [
        "### Custom NeRF Monitor Callback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1p1p2s9RVmN"
      },
      "source": [
        "def get_train_monitor(testDs, encoderFn, lxyz, lDir, imagePath):\n",
        "\t# grab images and rays from the testing dataset\n",
        "\t(tElements, tImages) = next(iter(testDs))\n",
        "\t(tRaysOriCoarse, tRaysDirCoarse, tTvalsCoarse) = tElements\n",
        "\n",
        "\t# build the test coarse ray\n",
        "\ttRaysCoarse = (tRaysOriCoarse[..., None, :] + \n",
        "\t\t(tRaysDirCoarse[..., None, :] * tTvalsCoarse[..., None]))\n",
        "\n",
        "\t# positional encode the rays and direction vectors for the coarse\n",
        "\t# ray\n",
        "\ttRaysCoarse = encoderFn(tRaysCoarse, lxyz)\n",
        "\ttDirsCoarseShape = tf.shape(tRaysCoarse[..., :3])\n",
        "\ttDirsCoarse = tf.broadcast_to(tRaysDirCoarse[..., None, :],\n",
        "\t\tshape=tDirsCoarseShape)\n",
        "\ttDirsCoarse = encoderFn(tDirsCoarse, lDir)\n",
        "\n",
        "\tclass TrainMonitor(Callback):\n",
        "\t\tdef on_epoch_end(self, epoch, logs=None):\n",
        "\t\t\t# compute the coarse model prediction\n",
        "\t\t\t(tRgbCoarse, tSigmaCoarse) = self.model.coarseModel.predict(\n",
        "\t\t\t\t[tRaysCoarse, tDirsCoarse])\n",
        "\t\t\t\n",
        "\t\t\t# render the image from the model prediciton\n",
        "\t\t\ttRenderCoarse = self.model.renderImageDepth(rgb=tRgbCoarse,\n",
        "\t\t\t\tsigma=tSigmaCoarse, tVals=tTvalsCoarse)\n",
        "\t\t\t(tImageCoarse, _, tWeightsCoarse) = tRenderCoarse\n",
        "\n",
        "\t\t\t# compute the middle values of t vals\n",
        "\t\t\ttTvalsCoarseMid = (0.5 * \n",
        "\t\t\t\t(tTvalsCoarse[..., 1:] + tTvalsCoarse[..., :-1]))\n",
        "\n",
        "\t\t\t# apply hierarchical sampling and get the t vals for the \n",
        "\t\t\t# fine model\n",
        "\t\t\ttTvalsFine = self.model.samplePdf(\n",
        "\t\t\t\ttValsMid=tTvalsCoarseMid, weights=tWeightsCoarse,\n",
        "\t\t\t\tnF=self.model.nF)\n",
        "\t\t\ttTvalsFine = tf.sort(\n",
        "\t\t\t\ttf.concat([tTvalsCoarse, tTvalsFine], axis=-1),\n",
        "\t\t\t\taxis=-1)\n",
        "\n",
        "\t\t\t# build the fine rays and positional encode it\n",
        "\t\t\ttRaysFine = (tRaysOriCoarse[..., None, :] + \n",
        "\t\t\t\t(tRaysDirCoarse[..., None, :] * tTvalsFine[..., None])\n",
        "\t\t\t)\n",
        "\t\t\ttRaysFine = self.model.encoderFn(tRaysFine, lxyz)\n",
        "\t\t\t\n",
        "\t\t\t# build the fine directions and positional encode it\n",
        "\t\t\ttDirsFineShape = tf.shape(tRaysFine[..., :3])\n",
        "\t\t\ttDirsFine = tf.broadcast_to(tRaysDirCoarse[..., None, :],\n",
        "\t\t\t\tshape=tDirsFineShape)\n",
        "\t\t\ttDirsFine = self.model.encoderFn(tDirsFine, lDir)\n",
        "\n",
        "\t\t\t# compute the fine model prediction\n",
        "\t\t\ttRgbFine, tSigmaFine = self.model.fineModel.predict(\n",
        "\t\t\t\t[tRaysFine, tDirsFine])\n",
        "\t\t\t\n",
        "\t\t\t# render the image from the model prediction\n",
        "\t\t\ttRenderFine = self.model.renderImageDepth(rgb=tRgbFine,\n",
        "\t\t\t\tsigma=tSigmaFine, tVals=tTvalsFine)\n",
        "\t\t\t(tImageFine, tDepthFine, _) = tRenderFine\n",
        "\n",
        "\t\t\t# plot the coarse image, fine image, fine depth map and\n",
        "\t\t\t# target image\n",
        "\t\t\t(_, ax) = plt.subplots(nrows=1, ncols=4, figsize=(10, 10))\n",
        "\t\t\tax[0].imshow(array_to_img(tImageCoarse[0]))\n",
        "\t\t\tax[0].set_title(f\"Corase Image\")\n",
        "\n",
        "\t\t\tax[1].imshow(array_to_img(tImageFine[0]))\n",
        "\t\t\tax[1].set_title(f\"Fine Image\")\n",
        "\n",
        "\t\t\tax[2].imshow(array_to_img(tDepthFine[0, ..., None]), \n",
        "\t\t\t\tcmap=\"inferno\")\n",
        "\t\t\tax[2].set_title(f\"Fine Depth Image\")\n",
        "\n",
        "\t\t\tax[3].imshow(array_to_img(tImages[0]))\n",
        "\t\t\tax[3].set_title(f\"Real Image\")\n",
        "\n",
        "\t\t\tplt.savefig(f\"{imagePath}/{epoch:03d}.png\")\n",
        "\t\t\tplt.close()\n",
        "\t\n",
        "\t# instantiate a train monitor callback\n",
        "\ttrainMonitor = TrainMonitor()\n",
        "\n",
        "\t# return the train monitor\n",
        "\treturn trainMonitor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2WwDbsWHu0t"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSg-3SvMSoWX"
      },
      "source": [
        "# get the train validation and test data\n",
        "print(\"[INFO] grabbing the data from json files...\")\n",
        "jsonTrainData = read_json(config.TRAIN_JSON)\n",
        "jsonValData = read_json(config.VAL_JSON)\n",
        "jsonTestData = read_json(config.TEST_JSON)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w46BB32iSrQP"
      },
      "source": [
        "focalLength = get_focal_from_fov(\n",
        "\tfieldOfView=jsonTrainData[\"camera_angle_x\"],\n",
        "\twidth=config.IMAGE_WIDTH)\n",
        "\n",
        "# print the focal length of the camera\n",
        "print(f\"[INFO] focal length of the camera: {focalLength}...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXzKcLrySuV7"
      },
      "source": [
        "# get the train, validation, and test image paths and camera2world\n",
        "# matrices\n",
        "print(\"[INFO] grabbing the image paths and camera2world matrices...\")\n",
        "trainImagePaths, trainC2Ws = get_image_c2w(jsonData=jsonTrainData,\n",
        "\tdatasetPath=config.DATASET_PATH)\n",
        "valImagePaths, valC2Ws = get_image_c2w(jsonData=jsonValData,\n",
        "\tdatasetPath=config.DATASET_PATH)\n",
        "testImagePaths, testC2Ws = get_image_c2w(jsonData=jsonTestData,\n",
        "\tdatasetPath=config.DATASET_PATH)\n",
        "\n",
        "# instantiate a object of our class used to load images from disk\n",
        "getImages = GetImages(imageHeight=config.IMAGE_HEIGHT,\n",
        "\timageWidth=config.IMAGE_WIDTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLujGFfVSwf0"
      },
      "source": [
        "# get the train, validation, and test image dataset\n",
        "print(\"[INFO] building the image dataset pipeline...\")\n",
        "trainImageDs = (\n",
        "\ttf.data.Dataset.from_tensor_slices(trainImagePaths)\n",
        "\t.map(getImages, num_parallel_calls=config.AUTO)\n",
        ")\n",
        "valImageDs = (\n",
        "\ttf.data.Dataset.from_tensor_slices(valImagePaths)\n",
        "\t.map(getImages, num_parallel_calls=config.AUTO)\n",
        ")\n",
        "testImageDs = (\n",
        "\ttf.data.Dataset.from_tensor_slices(testImagePaths)\n",
        "\t.map(getImages, num_parallel_calls=config.AUTO)\n",
        ")\n",
        "\n",
        "# instantiate the GetRays object\n",
        "getRays = GetRays(focalLength=focalLength, imageWidth=config.IMAGE_WIDTH,\n",
        "\timageHeight=config.IMAGE_HEIGHT, near=config.NEAR, far=config.FAR,\n",
        "\tnC=config.N_C)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0LCMjYJSyvb"
      },
      "source": [
        "# get the train validation and test rays dataset\n",
        "print(\"[INFO] building the rays dataset pipeline...\")\n",
        "trainRayDs = (\n",
        "\ttf.data.Dataset.from_tensor_slices(trainC2Ws)\n",
        "\t.map(getRays, num_parallel_calls=config.AUTO)\n",
        ")\n",
        "valRayDs = (\n",
        "\ttf.data.Dataset.from_tensor_slices(valC2Ws)\n",
        "\t.map(getRays, num_parallel_calls=config.AUTO)\n",
        ")\n",
        "testRayDs = (\n",
        "\ttf.data.Dataset.from_tensor_slices(testC2Ws)\n",
        "\t.map(getRays, num_parallel_calls=config.AUTO)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7sM-3ogS2By"
      },
      "source": [
        "# zip the images and rays dataset together\n",
        "trainDs = tf.data.Dataset.zip((trainRayDs, trainImageDs))\n",
        "valDs = tf.data.Dataset.zip((valRayDs, valImageDs))\n",
        "testDs = tf.data.Dataset.zip((testRayDs, testImageDs))\n",
        "\n",
        "# build data input pipeline for train, val, and test datasets\n",
        "trainDs = (\n",
        "\ttrainDs\n",
        "\t.shuffle(config.BATCH_SIZE)\n",
        "\t.batch(config.BATCH_SIZE)\n",
        "\t.repeat()\n",
        "\t.prefetch(config.AUTO)\n",
        ")\n",
        "valDs = (\n",
        "\tvalDs\n",
        "\t.shuffle(config.BATCH_SIZE)\n",
        "\t.batch(config.BATCH_SIZE)\n",
        "\t.repeat()\n",
        "\t.prefetch(config.AUTO)\n",
        ")\n",
        "testDs = (\n",
        "\ttestDs\n",
        "\t.batch(config.BATCH_SIZE)\n",
        "\t.prefetch(config.AUTO)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ts14Fe_lS66j"
      },
      "source": [
        "# instantiate the coarse model\n",
        "coarseModel = get_model(lxyz=config.L_XYZ, lDir=config.L_DIR,\n",
        "\tbatchSize=config.BATCH_SIZE, denseUnits=config.DENSE_UNITS,\n",
        "\tskipLayer=config.SKIP_LAYER)\n",
        "\n",
        "# instantiate the fine model\n",
        "fineModel = get_model(lxyz=config.L_XYZ, lDir=config.L_DIR,\n",
        "\tbatchSize=config.BATCH_SIZE, denseUnits=config.DENSE_UNITS,\n",
        "\tskipLayer=config.SKIP_LAYER)\n",
        "\n",
        "# instantiate the nerf trainer model\n",
        "nerfTrainerModel = Nerf_Trainer(coarseModel=coarseModel, fineModel=fineModel,\n",
        "\tlxyz=config.L_XYZ, lDir=config.L_DIR, encoderFn=encoder_fn,\n",
        "\trenderImageDepth=render_image_depth, samplePdf=sample_pdf,\n",
        "\tnF=config.N_F)\n",
        "\n",
        "# compile the nerf trainer model with Adam optimizer and MSE loss\n",
        "nerfTrainerModel.compile(optimizerCoarse=Adam(),optimizerFine=Adam(),\n",
        "\tlossFn=MeanSquaredError())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b19nQFEXHrrV"
      },
      "source": [
        "# check if the output image directory already exists, if it doesn't,\n",
        "# then create it\n",
        "if not os.path.exists(config.IMAGE_PATH):\n",
        "\tos.makedirs(config.IMAGE_PATH)\n",
        "\n",
        "# get the train monitor callback\n",
        "trainMonitorCallback = get_train_monitor(testDs=testDs,\n",
        "\tencoderFn=encoder_fn, lxyz=config.L_XYZ, lDir=config.L_DIR,\n",
        "\timagePath=config.IMAGE_PATH)\n",
        "\n",
        "# train the NeRF model\n",
        "print(\"[INFO] training the nerf model...\")\n",
        "nerfTrainerModel.fit(trainDs, steps_per_epoch=config.STEPS_PER_EPOCH,\n",
        "\tvalidation_data=valDs, validation_steps=config.VALIDATION_STEPS,\n",
        "\tepochs=config.EPOCHS, callbacks=[trainMonitorCallback],\n",
        ")\n",
        "\n",
        "# save the coarse and fine model\n",
        "nerfTrainerModel.coarseModel.save(config.COARSE_PATH)\n",
        "nerfTrainerModel.fineModel.save(config.FINE_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu5EMNFPH6Wf"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wS2o6EP3Nun0"
      },
      "source": [
        "# create a camera2world matrix list to store the novel view\n",
        "# camera2world matrices\n",
        "c2wList = []\n",
        "\n",
        "# iterate over theta and generate novel view camera2world matrices\n",
        "for theta in np.linspace(0.0, 360.0, config.SAMPLE_THETA_POINTS, \n",
        "\tendpoint=False):\n",
        "\t# generate camera2world matrix\n",
        "\tc2w = pose_spherical(theta, -30.0, 4.0)\n",
        "\t\n",
        "\t# append the new camera2world matrix into the collection\n",
        "\tc2wList.append(c2w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0JBdmuYTcxo"
      },
      "source": [
        "# get the train validation and test data\n",
        "print(\"[INFO] grabbing the data from json files...\")\n",
        "jsonTrainData = read_json(config.TRAIN_JSON)\n",
        "\n",
        "focalLength = get_focal_from_fov(\n",
        "\tfieldOfView=jsonTrainData[\"camera_angle_x\"],\n",
        "\twidth=config.IMAGE_WIDTH)\n",
        "\n",
        "# instantiate the GetRays object\n",
        "getRays = GetRays(focalLength=focalLength, imageWidth=config.IMAGE_WIDTH,\n",
        "\timageHeight=config.IMAGE_HEIGHT, near=config.NEAR, far=config.FAR,\n",
        "\tnC=config.N_C)\n",
        "\n",
        "# create a dataset from the novel view camera2world matrices\n",
        "ds = (\n",
        "\ttf.data.Dataset.from_tensor_slices(c2wList)\n",
        "\t.map(getRays)\n",
        "\t.batch(config.BATCH_SIZE)\n",
        ")\n",
        "\n",
        "# load the coarse and the fine model\n",
        "coarseModel = load_model(config.COARSE_PATH, compile=False)\n",
        "fineModel = load_model(config.FINE_PATH, compile=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmLiv5MqThMh"
      },
      "source": [
        "# create a list to hold all the novel view from the nerf model\n",
        "print(\"[INFO] grabbing the novel views...\")\n",
        "frameList = []\n",
        "for element in tqdm(ds):\n",
        "\t(raysOriCoarse, raysDirCoarse, tValsCoarse) = element\n",
        "\n",
        "\t# generate the coarse rays\n",
        "\traysCoarse = (raysOriCoarse[..., None, :] + \n",
        "\t\t(raysDirCoarse[..., None, :] * tValsCoarse[..., None]))\n",
        "\n",
        "\t# positional encode the rays and dirs\n",
        "\traysCoarse = encoder_fn(raysCoarse, config.L_XYZ)\n",
        "\tdirCoarseShape = tf.shape(raysCoarse[..., :3])\n",
        "\tdirsCoarse = tf.broadcast_to(raysDirCoarse[..., None, :],\n",
        "\t\tshape=dirCoarseShape)\n",
        "\tdirsCoarse = encoder_fn(dirsCoarse, config.L_DIR)\n",
        "\n",
        "\t# compute the predictions from the coarse model\n",
        "\t(rgbCoarse, sigmaCoarse) = coarseModel.predict(\n",
        "\t\t[raysCoarse, dirsCoarse])\n",
        "\t\n",
        "\t# render the image from the predicitons\n",
        "\trenderCoarse = render_image_depth(rgb=rgbCoarse,\n",
        "\t\tsigma=sigmaCoarse, tVals=tValsCoarse)\n",
        "\t(_, _, weightsCoarse) = renderCoarse\n",
        "\n",
        "\t# compute the middle values of t vals\n",
        "\ttValsCoarseMid = (0.5 * \n",
        "\t\t(tValsCoarse[..., 1:] + tValsCoarse[..., :-1]))\n",
        "\n",
        "\t# apply hierarchical sampling and get the t vals for the fine\n",
        "\t# model\n",
        "\ttValsFine = sample_pdf(tValsMid=tValsCoarseMid,\n",
        "\t\tweights=weightsCoarse, nF=config.N_F)\n",
        "\ttValsFine = tf.sort(\n",
        "\t\ttf.concat([tValsCoarse, tValsFine], axis=-1), axis=-1)\n",
        "\n",
        "\t# build the fine rays and positional encode it\n",
        "\traysFine = (raysOriCoarse[..., None, :] + \n",
        "\t\t(raysDirCoarse[..., None, :] * tValsFine[..., None]))\n",
        "\traysFine = encoder_fn(raysFine, config.L_XYZ)\n",
        "\t\n",
        "\t# build the fine direcitons and positional encode it\n",
        "\tdirsFineShape = tf.shape(raysFine[..., :3])\n",
        "\tdirsFine = tf.broadcast_to(raysDirCoarse[..., None, :],\n",
        "\t\tshape=dirsFineShape)\n",
        "\tdirsFine = encoder_fn(dirsFine, config.L_DIR)\n",
        "\n",
        "\t# compute the predictions from the fine model\n",
        "\t(rgbFine, sigmaFine) = fineModel.predict([raysFine, dirsFine])\n",
        "\t\n",
        "\t# render the image from the predicitons\n",
        "\trenderFine = render_image_depth(rgb=rgbFine, sigma=sigmaFine,\n",
        "\t\ttVals=tValsFine)\n",
        "\t(imageFine, _, _) = renderFine\n",
        "\n",
        "\t# insert the renderd fine image to the collection\n",
        "\tframeList.append(imageFine.numpy()[0])\n",
        "\n",
        "# check if the output video directory exists, if it does not, then\n",
        "# create it\n",
        "if not os.path.exists(config.VIDEO_PATH):\n",
        "\tos.makedirs(config.VIDEO_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YVGatq8Ti1e"
      },
      "source": [
        "# build the video from the frames and save it to disk\n",
        "print(\"[INFO] creating the video from the frames...\")\n",
        "imageio.mimwrite(config.OUTPUT_VIDEO_PATH, frameList, fps=config.FPS,\n",
        "\tquality=config.QUALITY, macro_block_size=config.MACRO_BLOCK_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrwInoPgWFzf"
      },
      "source": [
        "### Visualize the rendered 3D scene"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4LB9xLEVrQ9"
      },
      "source": [
        "mp4 = open(config.OUTPUT_VIDEO_PATH, \"rb\").read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls autoplay loop>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ogkNauArL6u"
      },
      "source": [
        "For a detailed walkthrough of the concepts and code, be sure to refer to the full tutorial, *Computer Graphics and Deep Learning with NeRF using TensorFlow and Keras* Series.\n",
        "- [*Part 1*](https://www.pyimagesearch.com/2021/11/10/computer-graphics-and-deep-learning-with-nerf-using-tensorflow-and-keras-part-1/) published on 2021-11-10.\n",
        "- [*Part 2*](https://www.pyimagesearch.com/2021/11/17/computer-graphics-and-deep-learning-with-nerf-using-tensorflow-and-keras-part-2/) published on 2021-11-17\n",
        "- [*Part 3*](https://www.pyimagesearch.com/2021/11/24/computer-graphics-and-deep-learning-with-nerf-using-tensorflow-and-keras-part-3/) published on 2021-11-24"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sef8alx4cGYc"
      },
      "source": [
        "# Code License Agreement\n",
        "```\n",
        "Copyright (c) 2021 PyImageSearch.com\n",
        "\n",
        "SIMPLE VERSION\n",
        "Feel free to use this code for your own projects, whether they are\n",
        "purely educational, for fun, or for profit. THE EXCEPTION BEING if\n",
        "you are developing a course, book, or other educational product.\n",
        "Under *NO CIRCUMSTANCE* may you use this code for your own paid\n",
        "educational or self-promotional ventures without written consent\n",
        "from Adrian Rosebrock and PyImageSearch.com.\n",
        "\n",
        "LONGER, FORMAL VERSION\n",
        "Permission is hereby granted, free of charge, to any person obtaining\n",
        "a copy of this software and associated documentation files\n",
        "(the \"Software\"), to deal in the Software without restriction,\n",
        "including without limitation the rights to use, copy, modify, merge,\n",
        "publish, distribute, sublicense, and/or sell copies of the Software,\n",
        "and to permit persons to whom the Software is furnished to do so,\n",
        "subject to the following conditions:\n",
        "The above copyright notice and this permission notice shall be\n",
        "included in all copies or substantial portions of the Software.\n",
        "Notwithstanding the foregoing, you may not use, copy, modify, merge,\n",
        "publish, distribute, sublicense, create a derivative work, and/or\n",
        "sell copies of the Software in any work that is designed, intended,\n",
        "or marketed for pedagogical or instructional purposes related to\n",
        "programming, coding, application development, or information\n",
        "technology. Permission for such use, copying, modification, and\n",
        "merger, publication, distribution, sub-licensing, creation of\n",
        "derivative works, or sale is expressly withheld.\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n",
        "EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES\n",
        "OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n",
        "NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n",
        "BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n",
        "ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
        "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\n",
        "```"
      ]
    }
  ]
}